# Adabelief-Optimizer
Repository for NeurIPS 2020 Spotlight  "AdaBelief Optimizer: Adapting stepsizes by the belief in observed gradients". Propose an optimizer that trains fast as Adam, generalizes well as SGD, and is stable to train GANs. 

See project page: https://juntang-zhuang.github.io/adabelief/

# Install from pip
```
pip install adabelief-pytorch
```
```
pip install adabelief-tf
```
```
pip install ranger-adabelief
```
